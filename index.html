
<!DOCTYPE html>
<html>

<head>
	<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-V3D20SW4N4"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-V3D20SW4N4');
</script>
	
	<meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, shrink-to-fit=no">
	<title>MMRel: A Relation Understanding Dataset and Benchmark in the MLLM Era</title>
  <link rel="stylesheet" href="assets/css/Highlight-Clean.css">
  <link rel="stylesheet" href="assets/css/styles.css">
  <script type="module" src="https://unpkg.com/@google/model-viewer@2.0.1/dist/model-viewer.min.js"></script>
  <link rel="manifest" href="site.webmanifest">

  <meta property="og:site_name" content="MMRel" />
  <meta property="og:title" content="MMRel: A Relation Understanding Dataset and Benchmark in the MLLM Era" />
  <meta property="og:description" content="MMRel: A Relation Understanding Dataset and Benchmark in the MLLM Era" />
  <script type="module" src="https://unpkg.com/@google/model-viewer@2.0.1/dist/model-viewer.min.js"></script>

  <link rel="stylesheet" href="assets/css/bootstrap.min.css">
  <link rel="stylesheet" href="assets/css/font-awesome.min.css">
  <link rel="stylesheet" href="assets/css/codemirror.min.css">
  <link rel="stylesheet" href="assets/css/app.css">

  <script src="assets/js/jquery.min.js"></script>
  <script src="assets/js/bootstrap.min.js"></script>
  <script src="assets/js/codemirror.min.js"></script>
  <script src="assets/js/clipboard.min.js"></script>
  <script src="assets/js/video_comparison.js"></script>
  <script src="assets/js/app.js"></script>

</head>

<body>
  <div class="highlight-clean" style="padding-bottom: 10px;">
  <div class="container" style="max-width: 950px;">
    <div class="column has-text-centered">
        <h1 class="col-md-12 text-center" id="title">
          <img src="./image/logo.png" height="200px">
        </h1>
    </div>
  </div>
	<div class="container" style="max-width: 950px;">
    <div class="row" id="title-row" style="max-width: 100%; margin: 0 auto; display: inline-block">
        <h1 class="col-md-12 text-center" id="title">
          <b>MMRel</b>: A Relation Understanding Benchmark in the MLLM Era <br>
            <small>
                arXiv 2024
            </small>
        </h1>
    </div>
	</div>
	<div class="container" style="max-width: 950px;">
		<div class="row" id="author-row" style="margin:0 auto;">
      <div class="col-md-12 text-center" style="display: table; margin:0 auto">
          <table class="author-table" id="author-table">
              <tr>
                  <td>
                      <a style="text-decoration:none" href="https://scholar.google.com/citations?user=LGM10RQAAAAJ&hl=zh-CN&inst=8669986779262753491&oi=ao">
                        <b>Jiahao Nie *</b>
                      </a>
                      <br>Nanyang Technological University
                  </td>
                  <td>
                      <a style="text-decoration:none" href="https://scholar.google.com/citations?user=sRBTPp4AAAAJ&hl=zh-CN&inst=8669986779262753491&oi=ao">
                        <b>Gongjie Zhang *</b>
                      </a>
                      <br>Alibaba DAMO Academy
                  </td>
                  <td>
                      <a style="text-decoration:none" href="https://scholar.google.com/citations?user=BpkQZGgAAAAJ&hl=zh-CN&inst=8669986779262753491&oi=ao">
                        <b>Wenbin An</b>
                      </a>
                      <br>Xi'an Jiaotong University
                  </td>
              </tr>
              <tr>
                <td>
                    <a style="text-decoration:none" href="https://scholar.google.com/citations?user=t9EqYQIAAAAJ&hl=zh-CN&inst=8669986779262753491&oi=ao">
                      <b>Yap-Peng Tan</b>
                    </a>
                    <br>Nanyang Technological University
                </td>
                <td>
                    <a style="text-decoration:none" href="https://scholar.google.com/citations?user=UGZXLxIAAAAJ&hl=zh-CN&inst=8669986779262753491&oi=ao">
                      <b>Alex C. Kot</b>
                    </a>
                    <br>Nanyang Technological University
                </td>
                <td>
                  <a style="text-decoration:none" href="https://scholar.google.com/citations?user=uYmK-A0AAAAJ&hl=zh-CN&inst=8669986779262753491&oi=ao">
                    <b>Shijian Lu</b>
                  </a>
                  <br>Nanyang Technological University
                </td>
            </tr>
          </table>
      </div>
  </div>   
	</div>
  <div class="row">
    <div class="col-sm-6 col-sm-offset-3 text-center">
        <ul class="nav nav-pills nav-justified">
            <li>
                <a href="https://arxiv.org/pdf/2406.09121">
                <img src="./image/arXiv.png" height="50px">
                    <h4><strong>Paper</strong></h4>
                </a>
            </li>
            <li>
                <a href="https://github.com/niejiahao1998/MMRel" target="_blank">
                <img src="./image/github.png" height="50px">
                    <h4><strong>Benchmark</strong></h4>
                </a>
            </li>
        </ul>
    </div>
  </div>
    </div>
    <div class="row">
      <div class="col-md-8 col-md-offset-2">
        <image src="./image/mmrel_sample.png" class="img-responsive" alt="overview" width="60%" style="width: 850px;margin:auto;">
    </div>
  </div>
    <hr class="divider" />
    <div class="container" style="max-width: 950px;">
        <div class="row">
            <div class="col-md-12">
                <h2><b>Abstract</b></h2>
                <p class="text-justify">
                  Though Multi-modal Large Language Models (MLLMs) have recently achieved significant progress, they often face various problems while handling inter-object relations, \ie, the interaction or association among distinct objects. This constraint largely stems from insufficient training and evaluation data for relation understanding, which has greatly impeded MLLMs in various vision-language generation and reasoning tasks. We attempt to address this challenge by introducing <b>M</b>ulti-<b>M</b>odal <b>Rel</b>ation Understanding (<b>MMRel</b>), a benchmark that features large-scale, high-quality, and diverse data on inter-object relations. MMRel features three distinctive attributes: <i>(i)</i> It contains over <i>22K</i> question-answer pairs, spanning three distinct domains and covering three relation categories, ensuring both scale and diversity; <i>(ii)</i> it provides manually verified, high-quality labels to ensure exceptional annotation accuracy; <i>(iii)</i> it includes adversarial cases with highly unusual relations, offering a challenging setting for evaluating relation hallucination. These features make MMRel ideal for evaluating MLLMs on relation understanding, as well as for fine-tuning MLLMs to enhance relation comprehension capability. Extensive experiments verify the effectiveness of MMRel in evaluating and enhancing MLLMs' relation understanding capabilities.
        </p>
        </div>
        </div>
    </div>

    
    <hr class="divider" />
    <div class="container" style="max-width: 950px;">
        <div class="row">
            <div class="col-md-12">
                <h2><b>Existing Benchmark</b></h2>
                <image src="image/existing_benchmark1.png" class="img-responsive" alt="overview" width="60%" style="width: 800px;margin:auto;"></image>
                <image src="image/existing_benchmark2.png" class="img-responsive" alt="overview" width="60%" style="width: 830px;margin:auto;"></image>
                <p class="text-justify">
                  Though several benchmarks on inter-object relations have been created, they were not intended for assessing MLLMs' relation understanding capabilities. Specifically, most existing benchmarks suffer from obvious limitations in data scales, relation categories, and data diversity. We address this issue by creating a comprehensive benchmark on inter-object relations, aiming to gauge and enhance MLLMs' relation understanding capability in various multimodal tasks.
                </p>
            </div>
        </div>
    </div>


    <hr class="divider" />
    <div class="container" style="max-width: 950px;">
        <div class="row">
            <div class="col-md-12">
                <h2><b>MMRel</b></h2>
                <image src="image/semidc.png" class="img-responsive" alt="overview" width="60%" style="width: 800px;margin:auto;"></image>
                <p class="text-justify">
                  We introduce a Semi-automatic Data Collection pipeline (SemiDC), which is capable of annotating large-scale existing images and generating a substantial amount of high-quality synthetic images. As discussed in paper, re-labeling existing images is essential since their original labels are incompatible with MLLMs. To this end, we design SemiDC to generate high-quality relation annotations via GPT-4V for large-scale VG benchmark. This process is divided into three stages: <i>(i)</i> Pre-processing: We selectively exclude images featuring complex scenes that pose challenges for GPT-4V in generating accurate annotations; <i>(ii)</i> Re-labeling via GPT-4V: We employ the in-context learning paradigm to use GPT-4V to generate relation annotations. GPe text prompt; <i>(iii)</i> Human verification: We manually assess and correct the annotations that are generated by GPT-4V, to ensure the quality of the collected inter-object relation data.
                </p>
                <image src="image/statistics.png" class="img-responsive" alt="overview" width="60%" style="width: 830px;margin:auto;"></image>
                <p class="text-justify">
                  Table shows the statistics of MMRel. Specifically, MMRel comprises around 22,500 question-answer pairs (15K Yes/No, and 7.5K Open-ended) across 7 subsets, spanning 3 domains and 3 categories of relations. Thanks to the open-vocabulary capability of GPT-4V, MMRel guarantees a diverse range of objects and action relations.
                </p>
            </div>
        </div>
    </div>


    <hr class="divider" />
    <div class="container" style="max-width: 950px;">
        <div class="row">
            <div class="col-md-12">
                <h2><b>Evaluation on MMRel</b></h2>
                <image src="image/evaluation.png" class="img-responsive" alt="overview" width="65%" style="width: 750px;margin:auto;"></image>
                <p class="text-justify">
                  We employ all of the 15K <i>Yes/No</i> question-answer pairs in MMRel to evaluate how MLLMs perform while handling multimodal data with rich inter-object relations. As Table shows, all nine MLLMs face various problems while handling relation understanding.
                </p>
            </div>
        </div>
    </div>
    
    <hr class="divider" />
    <div class="container" style="max-width: 950px;">
        <div class="row">
            <div class="col-md-12">
                <h2><b>Fine-Tuning with MMRel</b></h2>
                <image src="image/fine-tuning.png" class="img-responsive" alt="overview" width="60%" style="width: 450px;margin:auto;"></image>
                <p class="text-justify">
                  As Table shows, fine-tuning with MMRel improves the capabilities of relation understanding significantly and consistently across all data domains and relation categories. In addition, fine-tuning improves the relation understanding of the adversarial subset as well.
                </p>
            </div>
        </div>
    </div>
          

        
      

      </div>

    <hr class="divider" />
  
    <div class="container" style="max-width: 950px;">
        <div class="row">
            <div class="col-md-12">
                <h2><b>Citation</b></h2>
                Consider citing us if you find this project is helpful. <br>
                <code>
                    @article{nie2024mmrel,<br>
                    &nbsp; title={MMRel: A Relation Understanding Benchmark in the MLLM Era},<br>
                    &nbsp; author={Nie, Jiahao and Zhang, Gongjie and An, Wenbin and Tan, Yap-Peng and Kot, Alex C and Lu, Shijian},<br>
                    &nbsp; journal={arXiv preprint arXiv:2406.09121},<br>
                    &nbsp; year={2024}<br>
                  }</code></div>
        </div>
    </div>

    <hr class="divider" />
    <div class="container" style="max-width: 950px;">
      <div class="row">
          <div class="col-md-12">
              <h2>Acknowledgements</h2>
              <p>This webpage integrates components from many websites, including <a href="https://dorverbin.github.io/refnerf/">RefNeRF</a>, <a href="https://kunhao-liu.github.io/StyleRF/">StyleRF</a>, and <a href="https://richzhang.github.io/webpage-template/">Richard Zhang's template</a>.
                We sincerely thank the authors for their great work and websites.</p>
                <br></p>
            </div>
          </div>
    </div>

    <script src="https://polyfill.io/v3/polyfill.js?features=IntersectionObserver"></script>
    <script src="/assets/js/yall.js"></script>
    <script>
        yall(
            {
                observeChanges: true
            }
        );
    </script>
    <script src="/assets/js/scripts.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.5.0/js/bootstrap.bundle.min.js"></script>
    <script src="https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/js/webflow.fd002feec.js"></script>
    <!-- Import the component -->
</body>

</html>
